{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAM77TPepTJsWFdaXbc5ut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dovud-Asadi/-100-day-NLP-projects/blob/main/demo_multiclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "W7Q1JC-ofykp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j3IiMypofs2R"
      },
      "outputs": [],
      "source": [
        "# !pip install cohere\n",
        "# !pip install gradio\n",
        "# !pip install voyageai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import cohere\n",
        "import gradio as gr\n",
        "import voyageai"
      ],
      "metadata": {
        "id": "y4IXIBywf3Yz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **API**"
      ],
      "metadata": {
        "id": "xFG3EpvogJ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cohere API\n",
        "api_key_co = \"LpQudB9ymyh6GYotUB8NPcqE9A15YBz4XmCQewTJ\"\n",
        "#Voyager AI API\n",
        "api_key_vo = \"pa-08GVFImi310dmwU01vDx-Ue8VBNo36DhshchK0OTlIc\""
      ],
      "metadata": {
        "id": "-LpbqfOdgJbd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text_co(text, api_key):\n",
        "    co = cohere.Client(api_key)\n",
        "    response = co.embed(\n",
        "        texts=[text],\n",
        "        model=\"embed-multilingual-v3.0\",\n",
        "        input_type=\"classification\"\n",
        "    )\n",
        "    return response.embeddings[0]"
      ],
      "metadata": {
        "id": "cMLcDhtBgNca"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text_vo(text, api_key):\n",
        "    vo = voyageai.Client(api_key)\n",
        "    response = vo.embed(\n",
        "        texts=[text],\n",
        "        model=\"voyage-multilingual-2\",\n",
        "        input_type=\"document\"\n",
        "    )\n",
        "    return response.embeddings[0]"
      ],
      "metadata": {
        "id": "iCz8w1CUgObp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "Hae41-MmgXpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lowercase, remove punctuation (except '), and normalize whitespace.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation.replace(\"'\", \"\")))\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s\\']', '', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def preprocess_texts(text_list):\n",
        "    \"\"\"Apply `preprocess_text` to a list and join them.\"\"\"\n",
        "    return ' '.join(map(preprocess_text, text_list))\n",
        "\n",
        "def text_processing_pipeline(df):\n",
        "    def contains_04_0(text_list):\n",
        "        return any('04.0' in item for item in text_list)\n",
        "\n",
        "    def filter_long_texts(text_list):\n",
        "        return [text for text in text_list if len(text) > 4]\n",
        "\n",
        "    def keep_elements_starting_with_04(text_list):\n",
        "        return [element for element in text_list if element.startswith('04')]\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = text.replace(\"04.00.00.00 Oila qonunchiligi /\", \"\")\n",
        "        text = text.strip(\"[]\")\n",
        "        return text.strip()\n",
        "\n",
        "    def process_okoz_text(text):\n",
        "        if '/' in text:\n",
        "            text = text.split('/')[0].strip()\n",
        "        return text\n",
        "\n",
        "    def remove_duplicates(text_list):\n",
        "        return list(set(text_list))\n",
        "\n",
        "    def remove_semicolons(text_list):\n",
        "        return [text.replace(';', '') for text in text_list]\n",
        "\n",
        "    # Apply preprocessing steps\n",
        "    df = df[df['okoz_text'].apply(contains_04_0)].copy()\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(filter_long_texts)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(keep_elements_starting_with_04)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(lambda texts: [clean_text(text) for text in texts])\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(lambda texts: [process_okoz_text(text) for text in texts])\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_duplicates)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_semicolons)\n",
        "    df = df[df['okoz_text'].apply(len) == 1]\n",
        "    df.loc[:, 'processed_texts'] = df['related_texts'].apply(preprocess_texts)\n",
        "    df = df.drop(columns=['related_texts'])\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Combine the list elements into a single string\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "xTqsJgDYgYiX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, api_key, embedding):\n",
        "\n",
        "    df = text_processing_pipeline(df)\n",
        "\n",
        "    df.loc[:, 'embeddings'] = df['processed_texts'].apply(lambda text: embedding(text, api_key))\n",
        "\n",
        "    # Count the frequency of each class\n",
        "    class_counts = df['okoz_text'].value_counts()\n",
        "\n",
        "    # Assign labels based on frequency\n",
        "    class_threshold = 90\n",
        "    label_map = {}\n",
        "    label_counter = 1\n",
        "\n",
        "    def assign_label(class_name):\n",
        "        nonlocal label_counter\n",
        "        count = class_counts[class_name]\n",
        "        if count < class_threshold:\n",
        "            return 0\n",
        "        else:\n",
        "            if class_name not in label_map:\n",
        "                label_map[class_name] = label_counter\n",
        "                label_counter += 1\n",
        "            return label_map[class_name]\n",
        "\n",
        "    df.loc[:, 'label'] = df['okoz_text'].apply(assign_label)\n",
        "\n",
        "    # Select 150 rows with the longest text from class 4\n",
        "    class_4_df = df[df['label'] == 4].copy()\n",
        "    class_4_df.loc[:, 'text_length'] = class_4_df['okoz_text'].apply(len)\n",
        "    class_4_df = class_4_df.sort_values(by='text_length', ascending=False).head(150)\n",
        "    df = df[df['label'] != 4]\n",
        "    df = pd.concat([df, class_4_df.drop(columns=['text_length'])], ignore_index=True)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = ['processed_texts', 'okoz_text']\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Prepare the data for the model\n",
        "    X = np.array(df['embeddings'].tolist())\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader, df, label_map"
      ],
      "metadata": {
        "id": "P8V4rA7hgzV5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('/content/full_okoz.json')\n",
        "train_loader, test_loader, df, label_map = preprocess_data(df, api_key_vo, embed_text_vo)"
      ],
      "metadata": {
        "id": "r1ypO6n3g0h9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('embedded_text.csv', index=False)"
      ],
      "metadata": {
        "id": "x4C12OaJg22d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models**"
      ],
      "metadata": {
        "id": "_-XHdPn1jDFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 1**\n",
        "* with ELU\n",
        "* Accuracy: 77%"
      ],
      "metadata": {
        "id": "frLtkgPOmzL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassClassificationModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassClassificationModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\n",
        "        self.layer3 = nn.Linear(1024, 2048)\n",
        "        self.bn3 = nn.BatchNorm1d(2048)\n",
        "        self.layer4 = nn.Linear(2048, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.layer5 = nn.Linear(1024, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.dropout2 = nn.Dropout(p=0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.elu(self.bn1(self.layer1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.elu(self.bn2(self.layer2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.elu(self.bn3(self.layer3(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.elu(self.bn4(self.layer4(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.elu(self.bn5(self.layer5(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "T_V9cm-ni99Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "XZz_tjYLjmGy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = np.array(df['embeddings'][1]).shape[0]\n",
        "num_classes = 5\n",
        "model = MulticlassClassificationModel(input_dim, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "w98cwuOijNsW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "3SCZtr5PjR0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, criterion, optimizer, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())  # Also ensure labels are Long type\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        # scheduler.step()\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "l0SgTF9QjPVe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluating**"
      ],
      "metadata": {
        "id": "CQliWaI8j0Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "QuyBpAMIjzs_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer, num_epochs=50)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "311nRJu-j6FJ",
        "outputId": "ebd7aee6-8139-4f03-b57f-104f01cfbd20"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.6555\n",
            "Epoch 11/50, Loss: 0.0780\n",
            "Epoch 21/50, Loss: 0.0315\n",
            "Epoch 31/50, Loss: 0.0339\n",
            "Epoch 41/50, Loss: 0.0354\n",
            "Training Complete\n",
            "Test Accuracy: 77.78%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77.77777777777777"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 2**\n",
        "* ReLU\n",
        "* Accuracy: 75%"
      ],
      "metadata": {
        "id": "1VvLLtgbnISG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassClassificationModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassClassificationModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\n",
        "        self.layer3 = nn.Linear(1024, 2048)\n",
        "        self.bn3 = nn.BatchNorm1d(2048)\n",
        "        self.layer4 = nn.Linear(2048, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.layer5 = nn.Linear(1024, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.dropout2 = nn.Dropout(p=0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.layer1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.relu(self.bn2(self.layer2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.relu(self.bn3(self.layer3(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.relu(self.bn4(self.layer4(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.relu(self.bn5(self.layer5(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RQBb06ThnMr4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = np.array(df['embeddings'][1]).shape[0]\n",
        "num_classes = 5\n",
        "model = MulticlassClassificationModel(input_dim, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ],
      "metadata": {
        "id": "ipRkm0INndSN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        scheduler.step(epoch_loss)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "            print(f'Current Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "lRQ8sHFongU3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "DD9_0S_nniNP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=50)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb82UM9Lnjjf",
        "outputId": "06d3f16a-ef33-45c3-d94f-df2100ba3f14"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.0446\n",
            "Current Learning Rate: 0.000500\n",
            "Epoch 11/50, Loss: 0.0184\n",
            "Current Learning Rate: 0.000125\n",
            "Epoch 21/50, Loss: 0.0141\n",
            "Current Learning Rate: 0.000063\n",
            "Epoch 31/50, Loss: 0.0332\n",
            "Current Learning Rate: 0.000016\n",
            "Epoch 41/50, Loss: 0.0133\n",
            "Current Learning Rate: 0.000008\n",
            "Training Complete\n",
            "Test Accuracy: 75.40%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75.39682539682539"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 3**\n",
        "* Simpler Model\n",
        "* Accuracy 74%"
      ],
      "metadata": {
        "id": "pblFl3C4oV_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedMulticlassModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(OptimizedMulticlassModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.layer3 = nn.Linear(1024, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.layer1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batch_norm2(self.layer2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uc8ssahkoVub"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = np.array(df['embeddings'][1]).shape[0]\n",
        "model = OptimizedMulticlassModel(input_size=input_dim, num_classes=num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')"
      ],
      "metadata": {
        "id": "4PfehQELoekm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step(loss)\n",
        "        running_loss += loss.item()\n",
        "        if epoch % 10 == 0:\n",
        "          print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.6f}\")\n",
        "\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "wb8ZWPdhooid"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Test Loss: {avg_loss:.4f}\")\n",
        "    return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "4fbd23uColrQ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=50)\n",
        "evaluate_model(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHCQqi9hokn0",
        "outputId": "45cd662b-db67-4ec0-d4b7-6f487837485a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.0007\n",
            "Epoch 11/50, Loss: 0.0083\n",
            "Epoch 21/50, Loss: 0.0012\n",
            "Epoch 31/50, Loss: 0.0184\n",
            "Epoch 41/50, Loss: 0.0242\n",
            "Training Complete\n",
            "Test Accuracy: 74.60%, Test Loss: 0.0568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74.60317460317461, 0.056792984879206095)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 4**\n",
        "* With 100 epoch\n",
        "* Accuracy 78%"
      ],
      "metadata": {
        "id": "b35Q-ODNpPdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassClassificationModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassClassificationModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\n",
        "        self.layer3 = nn.Linear(1024, 2048)\n",
        "        self.bn3 = nn.BatchNorm1d(2048)\n",
        "        self.layer4 = nn.Linear(2048, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.layer5 = nn.Linear(1024, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.dropout2 = nn.Dropout(p=0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.layer1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.relu(self.bn2(self.layer2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.relu(self.bn3(self.layer3(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.relu(self.bn4(self.layer4(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.relu(self.bn5(self.layer5(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AKS9nLOZpWQJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = np.array(df['embeddings'][1]).shape[0]\n",
        "num_classes = 5\n",
        "model = MulticlassClassificationModel(input_dim, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ],
      "metadata": {
        "id": "8sB2NNvMpc5x"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        scheduler.step(epoch_loss)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "            print(f'Current Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "Vqo4SBT5pgT2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "_nvLZc_Sph9K"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100)\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNrT3uIopjU8",
        "outputId": "61c0efb2-417d-48a8-a20b-1e4d74bebcf3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 1.0722\n",
            "Current Learning Rate: 0.001000\n",
            "Epoch 11/100, Loss: 0.1671\n",
            "Current Learning Rate: 0.000500\n",
            "Epoch 21/100, Loss: 0.0280\n",
            "Current Learning Rate: 0.000500\n",
            "Epoch 31/100, Loss: 0.0475\n",
            "Current Learning Rate: 0.000250\n",
            "Epoch 41/100, Loss: 0.0235\n",
            "Current Learning Rate: 0.000125\n",
            "Epoch 51/100, Loss: 0.0210\n",
            "Current Learning Rate: 0.000031\n",
            "Epoch 61/100, Loss: 0.0159\n",
            "Current Learning Rate: 0.000008\n",
            "Epoch 71/100, Loss: 0.0134\n",
            "Current Learning Rate: 0.000002\n",
            "Epoch 81/100, Loss: 0.0155\n",
            "Current Learning Rate: 0.000000\n",
            "Epoch 91/100, Loss: 0.0216\n",
            "Current Learning Rate: 0.000000\n",
            "Training Complete\n",
            "Test Accuracy: 78.57%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78.57142857142857"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Input hendling**"
      ],
      "metadata": {
        "id": "LBdI7X1Sk-1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(related_texts):\n",
        "    def preprocess_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation.replace(\"'\", \"\")))\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\']', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def preprocess_texts(text_list):\n",
        "        processed_texts = [preprocess_text(text) for text in text_list]\n",
        "        return ' '.join(processed_texts)\n",
        "\n",
        "    preprocessed_text = preprocess_texts(related_texts)\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "id": "y7svDsRTj_qb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "Fuqarolik holati dalolatnomalarini yozish organlarida ish yuritish tartibi to‘g‘risidagi yo‘riqnomaga kiritilayotgan o‘zgartirishlar va qo‘shimcha\n",
        "1. 5-bandning beshinchi xatboshisidagi “Vazirlar Mahkamasining 2022-yil 25-fevraldagi 89-son qarori bilan tasdiqlangan O‘limni qayd qilish bilan bog‘liq kompozit davlat xizmatlarini ko‘rsatishning ma’muriy reglamentiga” degan so‘zlar “Vazirlar Mahkamasining 2023-yil 20-oktabrdagi 550-son qarori bilan tasdiqlangan O‘limni qayd qilish bilan bog‘liq kompozit davlat xizmatlarini ko‘rsatishning ma’muriy reglamentiga” degan so‘zlar bilan almashtirilsin.\n",
        "2. 11-bandning uchinchi xatboshisi quyidagi tahrirda bayon etilsin:\n",
        "“Vazirlar Mahkamasining 2023-yil 20-oktabrdagi 550-son qarori bilan tasdiqlangan Fuqarolik holati dalolatnomalarini qayd etish qoidalarining 4-bandiga asosan FHDY organlari, O‘zbekiston Respublikasining konsullik muassasalari va fuqarolar yig‘inlari fuqarolik holati dalolatnomalarining yozuv blanklari va boshqa hujjatlar bilan Vazirlik, shuningdek O‘zbekiston Respublikasi Tashqi ishlar vazirligi tomonidan ta’minlanadi.”.\n",
        "3. 23-bandning ikkinchi xatboshisidan “seriyasi, tartib raqamlari qirqib olinib, dalolatnomaning birinchi nusxasiga yelimlab qo‘yiladi. Guvohnomalarning qolgan qismlari” degan so‘zlar chiqarib tashlansin.\n",
        "4. Quyidagi mazmundagi 351-band bilan to‘ldirilsin:\n",
        "“351. Ariza beruvchi bola tug‘ilganligi haqida guvohnomani olish uchun O‘zbekiston Respublikasi Yagona interaktiv davlat xizmatlari portali (bundan buyon matnda YIDXP deb yuritiladi) orqali elektron so‘rovnoma to‘ldiradi yoki bevosita FHDY organiga murojaat qiladi.\n",
        "Bunda, FHDY organi xodimi elektron so‘rovnoma kelib tushgandan keyin bir ish kuni ichida bolaga tug‘ilganlik haqida guvohnoma rasmiylashtiradi hamda uni ariza beruvchiga taqdim etadi.\n",
        "Ariza beruvchi xohishiga ko‘ra, tug‘ilganlik haqida guvohnomani pochta aloqasi orqali olishi mumkin. Bunda, pochta xarajatlari ariza beruvchi hisobidan qoplanadi.”.\n",
        "5. 38-bandning ikkinchi xatboshisi quyidagi tahrirda bayon etilsin:\n",
        "“Agar tug‘ilgan bolaning onasi voyaga yetmagan bo‘lsa, tug‘ilishni qayd etish bolaning tug‘ilganligi haqida tibbiy ma’lumotnoma hamda vasiylik va homiylik organining arizasiga muvofiq FHDY organi tomonidan qayd etiladi. Bunda, bolani tuqqan onaning shaxsini tasdiqlovchi hujjati yoki tug‘ilganlik haqidagi guvohnomasi bilan o‘qish joyidan ma’lumotnoma taqdim etilishi lozim.”.\n",
        "6. 65-bandning ikkinchi xatboshisidagi “Yagona interaktiv davlat xizmatlari portali (bundan buyon matnda YIDXP deb yuritiladi)” degan so‘zlar “YIDXP” qisqartmasi bilan almashtirilsin.\n",
        "7. 102-bandning ikkinchi xatboshisi quyidagi tahrirda bayon etilsin:\n",
        "“O‘lim holati Vazirlar Mahkamasining 2023-yil 20-oktabrdagi 550-son qarori bilan tasdiqlangan O‘limni qayd qilish bilan bog‘liq kompozit davlat xizmatlarini ko‘rsatishning ma’muriy reglamentiga muvofiq elektron axborot tizimida kompozit davlat xizmati shaklida qayd etilganda, tibbiyot muassasalari tomonidan o‘lim holati FHDY organlarida qayd qilinganligi haqida QR-kod (matrik shtrixli kod) tasviri tushirilgan ma’lumotnoma ariza beruvchilarga ikki nusxada chop etib beriladi.”.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5oZK3IvllN62"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification function\n",
        "def classify_text(input_text, model, label_map, api_key):\n",
        "    preprocessed_text = process_text(input_text)\n",
        "    embedding = embed_text_co(preprocessed_text, api_key)\n",
        "    tensor = torch.tensor(embedding, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tensor = tensor.to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)\n",
        "        probabilities = torch.softmax(output, dim=1).cpu().numpy()\n",
        "\n",
        "    inverted_label_map = {v: k for k, v in label_map.items()}\n",
        "    class_probabilities = {inverted_label_map.get(i, \"other\"): prob for i, prob in enumerate(probabilities[0])}\n",
        "    sorted_class_probabilities = dict(sorted(class_probabilities.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    return sorted_class_probabilities"
      ],
      "metadata": {
        "id": "8LXB_pddlUar"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_probabilities = classify_text(input_text, model, label_map, api_key_co)\n",
        "for i in class_probabilities:\n",
        "  print(f\"{i} {class_probabilities[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjrpEw9bldgm",
        "outputId": "50093569-1215-44a3-e801-ab2f95cdbfb0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04.05.00.00 Ota-onalar qarovisiz qolgan bolalarni tarbiyalash shakllari 0.6089843511581421\n",
            "04.08.00.00 Oila, onalik, otalik va bolalikni himoya qilish va ijtimoiy qo‘llab-quvvatlash 0.272761732339859\n",
            "04.06.00.00 Fuqarolik holati dalolatnomalarini qayd qilish (shuningdek, 03.02.08.00ga qarang) 0.05793171375989914\n",
            "04.02.00.00 Nikoh 0.04897312447428703\n",
            "other 0.011349089443683624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRtAnl-amPDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}